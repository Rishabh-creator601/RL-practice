{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_input_neurons = len(env.reset())\n",
    "num_ouptut_neurons = env.action_space.n\n",
    "hidden_layer_dimensions = [16, 32, 32]\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "# training logic configurations\n",
    "max_num_episodes = 700\n",
    "buffer_size = 10000\n",
    "mini_batch_size = 64\n",
    "steps_per_target_update = 7\n",
    "consecutive_successful_episodes_to_stop = 20\n",
    "min_average_reward_for_stopping = 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,\n",
    "                 session,\n",
    "                 scope_name,\n",
    "                 input_size,\n",
    "                 hidden_layer_sizes,\n",
    "                 output_size,\n",
    "                 learning_rate):\n",
    "        \n",
    "        self.session = session\n",
    "        self.scope_name = scope_name\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.input = tf.placeholder(shape=[None, self.input_size],\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "            net = self.input\n",
    "            for layer_num, layer_size in enumerate(self.hidden_layer_sizes):\n",
    "                net = tf.layers.dense(net,\n",
    "                                      layer_size,\n",
    "                                      activation=tf.nn.relu)\n",
    "\n",
    "            self.output = tf.layers.dense(net,\n",
    "                                          self.output_size)\n",
    "\n",
    "            # Placeholder for expected q-values\n",
    "            self.y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "\n",
    "            # Using the loss method provided by tf directly\n",
    "            self.loss = tf.losses.mean_squared_error(self.y, self.output)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(self.loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.session.run(self.output,\n",
    "                                feed_dict={self.input: np.reshape(state, [-1, self.input_size])})\n",
    "    \n",
    "    def update(self, state, y):\n",
    "        return self.session.run([self.loss, self.optimizer],\n",
    "                              feed_dict={\n",
    "                                    self.input: state,\n",
    "                                    self.y: y\n",
    "                              })\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_copy_operations(source_scope, dest_scope):\n",
    "        source_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=source_scope)\n",
    "        dest_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=dest_scope)\n",
    "        \n",
    "        assert len(source_vars) == len(dest_vars)\n",
    "        result = []\n",
    "        \n",
    "        for source_var, dest_var in zip(source_vars, dest_vars):\n",
    "            result.append(dest_var.assign(source_var.value()))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(main_dqn, target_dqn, mini_batch):\n",
    "    \"\"\"\n",
    "    param: mini_batch:  A list of experiences in the form of\n",
    "                       `(state, action, reward, next_state, done)`\n",
    "    \"\"\"\n",
    "    states = [x[0] for x in mini_batch]\n",
    "    actions = [x[1] for x in mini_batch]\n",
    "    rewards = [x[2] for x in mini_batch]\n",
    "    next_states = [x[3] for x in mini_batch]\n",
    "    done = [x[4] for x in mini_batch]\n",
    "    \n",
    "    states = np.vstack(states)\n",
    "    next_states = np.vstack(next_states)\n",
    "    \n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    done = np.array(done)\n",
    "    \n",
    "    target_output = target_dqn.predict(next_states)\n",
    "    target_q_vals = rewards + gamma * np.max(target_output, axis=1) * (1 - done)\n",
    "    \n",
    "    main_output = main_dqn.predict(states)\n",
    "    main_output[np.arange(len(states)), actions] = target_q_vals\n",
    "    final_target_q_values = main_output\n",
    "    \n",
    "    loss, optimizer = main_dqn.update(states, final_target_q_values)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved after 348 episodes\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "last_n_rewards = deque(maxlen=consecutive_successful_episodes_to_stop)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    main_dqn = DQN(session=sess,\n",
    "                   scope_name=\"q_main\",\n",
    "                   input_size=num_input_neurons,\n",
    "                   hidden_layer_sizes=hidden_layer_dimensions,\n",
    "                   output_size=num_ouptut_neurons,\n",
    "                   learning_rate=learning_rate)\n",
    "    \n",
    "    target_dqn = DQN(session=sess,\n",
    "                     scope_name=\"q_target\",\n",
    "                     input_size=num_input_neurons,\n",
    "                     hidden_layer_sizes=hidden_layer_dimensions,\n",
    "                     output_size=num_ouptut_neurons,\n",
    "                     learning_rate=learning_rate)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Make them identical to begin with\n",
    "    sess.run(DQN.create_copy_operations(\"q_main\", \"q_target\"))\n",
    "    \n",
    "    for ep_num in range(max_num_episodes):\n",
    "               \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward, steps = 0, 0\n",
    "        \n",
    "        # epsilon decay\n",
    "        epsilon = 1. /((ep_num / 10) + 1)\n",
    "        \n",
    "        while not done:\n",
    "            # select the action\n",
    "            action = None\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(main_dqn.predict(state))\n",
    "            \n",
    "            # execute the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if done:\n",
    "                reward = -1\n",
    "            \n",
    "            # add experience to the buffer\n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # sample from the buffer and train\n",
    "            if len(replay_buffer) > mini_batch_size:\n",
    "                mini_batch = random.sample(replay_buffer, mini_batch_size)\n",
    "                # Feeds samples to the networks, computes the loss, and updates\n",
    "                # the weights\n",
    "                loss = train_dqn(main_dqn, target_dqn, mini_batch)\n",
    "            \n",
    "            # Copy weights every `steps_per_target_update` iterations\n",
    "            if steps % steps_per_target_update == 0:\n",
    "                sess.run(DQN.create_copy_operations(\"q_main\", \"q_target\"))    \n",
    "               \n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "\n",
    "        last_n_rewards.append(episode_reward)\n",
    "        last_n_avg_reward = np.mean(last_n_rewards)\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if len(last_n_rewards) == consecutive_successful_episodes_to_stop \\\n",
    "            and last_n_avg_reward > min_average_reward_for_stopping:\n",
    "                print(\"Solved after {} epsiodes\".format(ep_i))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packt_rl",
   "language": "python",
   "name": "packt_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
